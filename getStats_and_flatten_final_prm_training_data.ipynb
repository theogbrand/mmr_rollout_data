{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ace7f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0009238e-3943-4b93-8ebd-b127eafc5276', 'image_url': '/data/users/brandon/ob1-projects/InternVL/internvl_chat/rollout_generation/preprocessed_prompts/preprocessing_scripts/DVQA/subset_images/bar_train_00052205.png', 'conversations': [{'from': 'system', 'value': \"You are a Visual Reasoning Teacher. Given a visual reasoning question with provided images and a student's solution, evaluate the visual interpretation accuracy, logical consistency of the current step, and whether it will lead to the correct final solution.\"}, {'from': 'human', 'value': '### Question:\\nHow many people prefer the object vision in the category switch? Answer the question using a single word or phrase.\\n\\n### Solution Process:\\n[Visual Elements]\\n<step_1>\\nThe chart title is \"Most preferred objects of different categories\".\\n</step_1>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '<step_2>\\nThe y-axis is labeled \"Number of People\" and ranges from 0 to 10.\\n</step_2>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '<step_3>\\nThe x-axis categories are \"worry\", \"vision\", and \"denial\".\\n</step_3>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '<step_4>\\nThere are three legends: \"switch\" (blue), \"guy\" (red), and \"motel\" (yellow).\\n</step_4>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '<step_5>\\nEach category has three bars representing the preferences for \"switch\", \"guy\", and \"motel\".\\n</step_5>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '<step_6>\\nFor \"vision\", the blue bar (switch) height is 1.\\n</step_6>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '[Reasoning]\\n<step_1>\\nThe question asks for the number of people who prefer object \"switch\" in the category \"vision\".\\n</step_1>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '<step_2>\\nLocate the \"vision\" category on the x-axis and identify the blue bar (since \"switch\" is blue).\\n</step_2>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '<step_3>\\nThe blue bar\\'s height for \"vision\" is 1.\\n</step_3>'}, {'from': 'gpt', 'value': '<+>'}, {'from': 'human', 'value': '<step_4>\\nThus, the answer is 1 person.\\n</step_4>'}, {'from': 'gpt', 'value': '<+>'}], 'first_incorrect_step': None, 'steps_with_score': [{'step': 'The chart title is \"Most preferred objects of different categories\".', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'The y-axis is labeled \"Number of People\" and ranges from 0 to 10.', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'The x-axis categories are \"worry\", \"vision\", and \"denial\".', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'There are three legends: \"switch\" (blue), \"guy\" (red), and \"motel\" (yellow).', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'Each category has three bars representing the preferences for \"switch\", \"guy\", and \"motel\".', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'For \"vision\", the blue bar (switch) height is 1.', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'The question asks for the number of people who prefer object \"switch\" in the category \"vision\".', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'Locate the \"vision\" category on the x-axis and identify the blue bar (since \"switch\" is blue).', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'The blue bar\\'s height for \"vision\" is 1.', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}, {'step': 'Thus, the answer is 1 person.', 'score': 1.0, 'num_mc_correct': 16, 'num_mc_total': 16}], 'consensus_filtering_algo_label': 'o4-mini_correct_and_MC_agrees', 'verifier_identified_first_incorrect_step_solution': None}\n",
      "['id', 'image_url', 'conversations', 'first_incorrect_step', 'steps_with_score', 'consensus_filtering_algo_label', 'verifier_identified_first_incorrect_step_solution']\n"
     ]
    }
   ],
   "source": [
    "# load all JSONL files in the directory\n",
    "import os\n",
    "import json\n",
    "\n",
    "# TODO: Edit the MC last file path - then just Restart and Run All\n",
    "dir_path = \"/mnt/fast10/brandon/mmr_rollout_data/prm_training_data/debug/mc0.0\"\n",
    "\n",
    "first_jsonl = next((f for f in os.listdir(dir_path) if f.endswith(\".jsonl\")), None)\n",
    "if first_jsonl:\n",
    "    with open(os.path.join(dir_path, first_jsonl), \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line:\n",
    "            columns = list(json.loads(first_line).keys())\n",
    "            print(json.loads(first_line))\n",
    "        else:\n",
    "            columns = []\n",
    "else:\n",
    "    columns = []\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "701e5d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking schema consistency...\n",
      "✓ All files have consistent schema\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# First pass: Check schema consistency across all files\n",
    "print(\"Checking schema consistency...\")\n",
    "file_schemas = {}\n",
    "jsonl_files = [f for f in os.listdir(dir_path) if f.endswith(\".jsonl\")]\n",
    "\n",
    "for file in jsonl_files:\n",
    "    with open(os.path.join(dir_path, file), \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line:\n",
    "            schema = set(json.loads(first_line).keys())\n",
    "            file_schemas[file] = schema\n",
    "\n",
    "# Check if all schemas are identical\n",
    "reference_schema = next(iter(file_schemas.values()))\n",
    "inconsistent_files = []\n",
    "for file, schema in file_schemas.items():\n",
    "    if schema != reference_schema:\n",
    "        inconsistent_files.append((file, schema))\n",
    "\n",
    "if inconsistent_files:\n",
    "    print(\"WARNING: Schema inconsistencies found!\")\n",
    "    for file, schema in inconsistent_files:\n",
    "        print(f\"  {file}: {schema}\")\n",
    "        print(f\"    Missing: {reference_schema - schema}\")\n",
    "        print(f\"    Extra: {schema - reference_schema}\")\n",
    "else:\n",
    "    print(\"✓ All files have consistent schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3c5960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc_score: mc0.0\n"
     ]
    }
   ],
   "source": [
    "mc_score = dir_path.split(\"/\")[-1]\n",
    "print(f\"mc_score: {mc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f21d6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 6 files...\n",
      "Processing dvqa_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl...\n",
      "  Lines processed: 16663\n",
      "Processing AI2D_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl...\n",
      "  Lines processed: 22399\n",
      "Processing InfoVQA_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl...\n",
      "  Lines processed: 25518\n",
      "Processing CLEVR_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl...\n",
      "  Lines processed: 27326\n",
      "Processing vqav2_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl...\n",
      "  Lines processed: 8138\n",
      "Processing RAVEN_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl...\n",
      "  Lines processed: 203115\n",
      "\n",
      "Writing flattened data to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/debug/mc0.0/final_single_prm_training_data_mc0.0_v2.jsonl...\n",
      "✓ Flattened 303159 total records to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/debug/mc0.0/final_single_prm_training_data_mc0.0_v2.jsonl\n",
      "\n",
      "============================================================\n",
      "STATISTICS BY FILE\n",
      "============================================================\n",
      "\n",
      "dvqa_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl:\n",
      "  Total records: 16663\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 1251 (29.6% of incorrect samples)\n",
      "    Visual Elements: 2982 (70.4% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 11942\n",
      "    o4-mini_correct_and_MC_disagrees: 488\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 4233\n",
      "  Training samples breakdown:\n",
      "    Used for training: 16175 (11942 correct + 4233 incorrect)\n",
      "    Not used for training: 488 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 73.8% correct, 26.2% incorrect\n",
      "\n",
      "AI2D_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl:\n",
      "  Total records: 22399\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 1280 (19.4% of incorrect samples)\n",
      "    Visual Elements: 5334 (80.6% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 14357\n",
      "    o4-mini_correct_and_MC_disagrees: 1428\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 6614\n",
      "  Training samples breakdown:\n",
      "    Used for training: 20971 (14357 correct + 6614 incorrect)\n",
      "    Not used for training: 1428 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 68.5% correct, 31.5% incorrect\n",
      "\n",
      "InfoVQA_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl:\n",
      "  Total records: 25518\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 709 (18.5% of incorrect samples)\n",
      "    Visual Elements: 3119 (81.5% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 17922\n",
      "    o4-mini_correct_and_MC_disagrees: 3768\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 3828\n",
      "  Training samples breakdown:\n",
      "    Used for training: 21750 (17922 correct + 3828 incorrect)\n",
      "    Not used for training: 3768 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 82.4% correct, 17.6% incorrect\n",
      "\n",
      "CLEVR_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl:\n",
      "  Total records: 27326\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 379 (3.7% of incorrect samples)\n",
      "    Visual Elements: 9755 (96.3% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 15343\n",
      "    o4-mini_correct_and_MC_disagrees: 1849\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 10134\n",
      "  Training samples breakdown:\n",
      "    Used for training: 25477 (15343 correct + 10134 incorrect)\n",
      "    Not used for training: 1849 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 60.2% correct, 39.8% incorrect\n",
      "\n",
      "vqav2_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl:\n",
      "  Total records: 8138\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 1014 (40.9% of incorrect samples)\n",
      "    Visual Elements: 1464 (59.1% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 4650\n",
      "    o4-mini_correct_and_MC_disagrees: 1010\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 2478\n",
      "  Training samples breakdown:\n",
      "    Used for training: 7128 (4650 correct + 2478 incorrect)\n",
      "    Not used for training: 1010 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 65.2% correct, 34.8% incorrect\n",
      "\n",
      "RAVEN_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.0.jsonl:\n",
      "  Total records: 203115\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 26752 (13.6% of incorrect samples)\n",
      "    Visual Elements: 170276 (86.4% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 1477\n",
      "    o4-mini_correct_and_MC_disagrees: 4610\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 197028\n",
      "  Training samples breakdown:\n",
      "    Used for training: 198505 (1477 correct + 197028 incorrect)\n",
      "    Not used for training: 4610 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 0.7% correct, 99.3% incorrect\n",
      "\n",
      "============================================================\n",
      "OVERALL STATISTICS\n",
      "============================================================\n",
      "Total records across all files: 303159\n",
      "Overall first incorrect step sections:\n",
      "  Reasoning: 31385 (14.0% of incorrect samples)\n",
      "  Visual Elements: 192930 (86.0% of incorrect samples)\n",
      "Overall consensus filtering labels:\n",
      "  o4-mini_correct_and_MC_agrees: 65691\n",
      "  o4-mini_correct_and_MC_disagrees: 13153\n",
      "  o4-mini_incorrect_and_MC_agrees_and_disagrees: 224315\n",
      "Overall training samples breakdown:\n",
      "  Used for training: 290006 (65691 correct + 224315 incorrect)\n",
      "  Not used for training: 13153 (o4-mini_correct_and_MC_disagrees)\n",
      "  Training split: 22.7% correct, 77.3% incorrect\n"
     ]
    }
   ],
   "source": [
    "# Statistics collection\n",
    "file_stats = {}\n",
    "all_data = []\n",
    "\n",
    "print(f\"\\nProcessing {len(jsonl_files)} files...\")\n",
    "\n",
    "for file in jsonl_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    \n",
    "    # Initialize stats for this file\n",
    "    first_incorrect_step_sections = Counter()\n",
    "    consensus_filtering_labels = Counter()\n",
    "    line_count = 0\n",
    "    \n",
    "    with open(os.path.join(dir_path, file), \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            all_data.append(item)\n",
    "            line_count += 1\n",
    "            \n",
    "            # Collect first_incorrect_step section distribution\n",
    "            if \"first_incorrect_step\" in item and item[\"first_incorrect_step\"] is not None:\n",
    "                if isinstance(item[\"first_incorrect_step\"], (list, tuple)) and len(item[\"first_incorrect_step\"]) >= 1:\n",
    "                    section_name = item[\"first_incorrect_step\"][0]\n",
    "                    first_incorrect_step_sections[section_name] += 1\n",
    "            \n",
    "            # Collect consensus_filtering_algo_label distribution\n",
    "            if \"consensus_filtering_algo_label\" in item and item[\"consensus_filtering_algo_label\"] is not None:\n",
    "                consensus_filtering_labels[item[\"consensus_filtering_algo_label\"]] += 1\n",
    "    \n",
    "    # Store stats for this file\n",
    "    file_stats[file] = {\n",
    "        \"line_count\": line_count,\n",
    "        \"first_incorrect_step_sections\": dict(first_incorrect_step_sections),\n",
    "        \"consensus_filtering_labels\": dict(consensus_filtering_labels)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Lines processed: {line_count}\")\n",
    "\n",
    "# TODO: CHANGE FILE NAME PATH!!!\n",
    "output_file = os.path.join(dir_path, f\"final_single_prm_training_data_{mc_score}_v2.jsonl\")\n",
    "print(f\"\\nWriting flattened data to {output_file}...\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Flattened {len(all_data)} total records to {output_file}\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICS BY FILE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for file, stats in file_stats.items():\n",
    "    print(f\"\\n{file}:\")\n",
    "    print(f\"  Total records: {stats['line_count']}\")\n",
    "    \n",
    "    # Get key counts for percentage calculations\n",
    "    incorrect_count = stats['consensus_filtering_labels'].get('o4-mini_incorrect_and_MC_agrees_and_disagrees', 0)\n",
    "    correct_count = stats['consensus_filtering_labels'].get('o4-mini_correct_and_MC_agrees', 0)\n",
    "    unused_count = stats['consensus_filtering_labels'].get('o4-mini_correct_and_MC_disagrees', 0)\n",
    "    \n",
    "    reasoning_count = stats['first_incorrect_step_sections'].get('Reasoning', 0)\n",
    "    visual_count = stats['first_incorrect_step_sections'].get('Visual Elements', 0)\n",
    "    \n",
    "    print(f\"  First incorrect step sections:\")\n",
    "    for section, count in sorted(stats['first_incorrect_step_sections'].items()):\n",
    "        if incorrect_count > 0:\n",
    "            pct = (count / incorrect_count) * 100\n",
    "            print(f\"    {section}: {count} ({pct:.1f}% of incorrect samples)\")\n",
    "        else:\n",
    "            print(f\"    {section}: {count} (no incorrect samples)\")\n",
    "    \n",
    "    print(f\"  Consensus filtering labels:\")\n",
    "    for label, count in sorted(stats['consensus_filtering_labels'].items()):\n",
    "        print(f\"    {label}: {count}\")\n",
    "    \n",
    "    # Training sample breakdown\n",
    "    training_total = correct_count + incorrect_count\n",
    "    print(f\"  Training samples breakdown:\")\n",
    "    print(f\"    Used for training: {training_total} ({correct_count} correct + {incorrect_count} incorrect)\")\n",
    "    print(f\"    Not used for training: {unused_count} (o4-mini_correct_and_MC_disagrees)\")\n",
    "    \n",
    "    if training_total > 0:\n",
    "        correct_pct = (correct_count / training_total) * 100\n",
    "        incorrect_pct = (incorrect_count / training_total) * 100\n",
    "        print(f\"    Training split: {correct_pct:.1f}% correct, {incorrect_pct:.1f}% incorrect\")\n",
    "    else:\n",
    "        print(f\"    Training split: No training samples\")\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_sections = Counter()\n",
    "all_labels = Counter()\n",
    "\n",
    "for stats in file_stats.values():\n",
    "    for section, count in stats['first_incorrect_step_sections'].items():\n",
    "        all_sections[section] += count\n",
    "    for label, count in stats['consensus_filtering_labels'].items():\n",
    "        all_labels[label] += count\n",
    "\n",
    "print(f\"Total records across all files: {len(all_data)}\")\n",
    "\n",
    "# Overall key counts for percentage calculations\n",
    "overall_incorrect_count = all_labels.get('o4-mini_incorrect_and_MC_agrees_and_disagrees', 0)\n",
    "overall_correct_count = all_labels.get('o4-mini_correct_and_MC_agrees', 0)\n",
    "overall_unused_count = all_labels.get('o4-mini_correct_and_MC_disagrees', 0)\n",
    "\n",
    "print(f\"Overall first incorrect step sections:\")\n",
    "for section, count in sorted(all_sections.items()):\n",
    "    if overall_incorrect_count > 0:\n",
    "        pct = (count / overall_incorrect_count) * 100\n",
    "        print(f\"  {section}: {count} ({pct:.1f}% of incorrect samples)\")\n",
    "    else:\n",
    "        print(f\"  {section}: {count} (no incorrect samples)\")\n",
    "\n",
    "print(f\"Overall consensus filtering labels:\")\n",
    "for label, count in sorted(all_labels.items()):\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "# Overall training sample breakdown\n",
    "overall_training_total = overall_correct_count + overall_incorrect_count\n",
    "print(f\"Overall training samples breakdown:\")\n",
    "print(f\"  Used for training: {overall_training_total} ({overall_correct_count} correct + {overall_incorrect_count} incorrect)\")\n",
    "print(f\"  Not used for training: {overall_unused_count} (o4-mini_correct_and_MC_disagrees)\")\n",
    "\n",
    "if overall_training_total > 0:\n",
    "    overall_correct_pct = (overall_correct_count / overall_training_total) * 100\n",
    "    overall_incorrect_pct = (overall_incorrect_count / overall_training_total) * 100\n",
    "    print(f\"  Training split: {overall_correct_pct:.1f}% correct, {overall_incorrect_pct:.1f}% incorrect\")\n",
    "else:\n",
    "    print(f\"  Training split: No training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bccd43cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            messages  \\\n",
      "0  [{'role': 'system', 'content': [{'type': 'text...   \n",
      "1  [{'role': 'system', 'content': [{'type': 'text...   \n",
      "2  [{'role': 'system', 'content': [{'type': 'text...   \n",
      "3  [{'role': 'system', 'content': [{'type': 'text...   \n",
      "4  [{'role': 'system', 'content': [{'type': 'text...   \n",
      "\n",
      "                                              images  \\\n",
      "0  [s3://arf-share/arf-ob1-mm-reasoning/training_...   \n",
      "1  [s3://arf-share/arf-ob1-mm-reasoning/training_...   \n",
      "2  [s3://arf-share/arf-ob1-mm-reasoning/training_...   \n",
      "3  [s3://arf-share/arf-ob1-mm-reasoning/training_...   \n",
      "4  [s3://arf-share/arf-ob1-mm-reasoning/training_...   \n",
      "\n",
      "                                     id  \n",
      "0  00205c45-6ef8-456c-9409-caee64c59695  \n",
      "1  0028c172-433c-45e0-85eb-0d037e11c28d  \n",
      "2  002b3294-1dde-4830-8526-79cfa8158ccd  \n",
      "3  0033324c-528f-4fce-8e58-40aa33710574  \n",
      "4  003fdba9-9908-4676-801d-b69df5a27483  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dir_path = f\"/mnt/fast10/brandon/mmr_rollout_data/prm_training_data/train/{mc_score}/vqav2_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_final_trl_format_{mc_score}.jsonl\"\n",
    "\n",
    "with open(dir_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "records = [json.loads(line) for line in lines if line.strip()]\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e7ce42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://arf-share/arf-ob1-mm-reasoning/training_data_images/VQAv2/subset_images/COCO_train2014_000000240080.jpg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"images\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f4409e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': [{'type': 'text',\n",
       "   'text': '<image>\\n### Question:\\nWhat year was this picture taken? Answer the question using a single word or phrase.\\n\\n### Solution Process:\\n[Visual Elements]\\n<step_1>\\nObserving several brown stuffed teddy bears, some with white paws, hanging close together, likely as display items.\\n</step_1>',\n",
       "   'index': None}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"messages\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd74b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/fast10/brandon/mmr_rollout_data/training_data_images/AI2D/subset_images/706.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd_abs_path = os.path.abspath(os.getcwd())\n",
    "test_path = \"s3://arf-share/arf-ob1-mm-reasoning/training_data_images/AI2D/subset_images/706.png\"\n",
    "test_path = test_path.replace(\"s3://arf-share/arf-ob1-mm-reasoning/\", cwd_abs_path + \"/\")\n",
    "print(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4ff84",
   "metadata": {},
   "source": [
    "# Run this part for merging the \"train\" TRL messages array directory\n",
    "- above runs merges for \"debug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076083ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mc_score = \"mc0.01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df7930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['messages', 'images', 'id']\n"
     ]
    }
   ],
   "source": [
    "# load all JSONL files in the directory\n",
    "import os\n",
    "import json\n",
    "\n",
    "dir_path = f\"/mnt/fast10/brandon/mmr_rollout_data/prm_training_data/train/{mc_score}\"\n",
    "\n",
    "first_jsonl = next((f for f in os.listdir(dir_path) if f.endswith(\".jsonl\")), None)\n",
    "if first_jsonl:\n",
    "    with open(os.path.join(dir_path, first_jsonl), \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line:\n",
    "            columns = list(json.loads(first_line).keys())\n",
    "        else:\n",
    "            columns = []\n",
    "else:\n",
    "    columns = []\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaa1364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking schema consistency...\n",
      "✓ All files have consistent schema\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# First pass: Check schema consistency across all files\n",
    "print(\"Checking schema consistency...\")\n",
    "file_schemas = {}\n",
    "jsonl_files = [f for f in os.listdir(dir_path) if f.endswith(\".jsonl\")]\n",
    "\n",
    "for file in jsonl_files:\n",
    "    with open(os.path.join(dir_path, file), \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line:\n",
    "            schema = set(json.loads(first_line).keys())\n",
    "            file_schemas[file] = schema\n",
    "\n",
    "# Check if all schemas are identical\n",
    "reference_schema = next(iter(file_schemas.values()))\n",
    "inconsistent_files = []\n",
    "for file, schema in file_schemas.items():\n",
    "    if schema != reference_schema:\n",
    "        inconsistent_files.append((file, schema))\n",
    "\n",
    "if inconsistent_files:\n",
    "    print(\"WARNING: Schema inconsistencies found!\")\n",
    "    for file, schema in inconsistent_files:\n",
    "        print(f\"  {file}: {schema}\")\n",
    "        print(f\"    Missing: {reference_schema - schema}\")\n",
    "        print(f\"    Extra: {schema - reference_schema}\")\n",
    "else:\n",
    "    print(\"✓ All files have consistent schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17b95a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing vqav2_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_final_trl_format_mc0.0.jsonl...\n",
      "  Lines processed: 7128\n",
      "Processing InfoVQA_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_final_trl_format_mc0.0.jsonl...\n",
      "  Lines processed: 21750\n",
      "Processing RAVEN_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_final_trl_format_mc0.0.jsonl...\n",
      "  Lines processed: 198505\n",
      "Processing dvqa_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_final_trl_format_mc0.0.jsonl...\n",
      "  Lines processed: 16175\n",
      "Processing AI2D_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_final_trl_format_mc0.0.jsonl...\n",
      "  Lines processed: 20971\n",
      "Processing CLEVR_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_final_trl_format_mc0.0.jsonl...\n",
      "  Lines processed: 25477\n",
      "\n",
      "Writing flattened data to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/train/mc0.0/final_flattened_trl_format_prm_training_data_500k_mc0.0_v2.jsonl...\n",
      "✓ Flattened 290006 total records to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/train/mc0.0/final_flattened_trl_format_prm_training_data_500k_mc0.0_v2.jsonl\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "\n",
    "for file in jsonl_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    \n",
    "    line_count = 0\n",
    "    \n",
    "    with open(os.path.join(dir_path, file), \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            all_data.append(item)\n",
    "            line_count += 1\n",
    "    print(f\"  Lines processed: {line_count}\")\n",
    "\n",
    "# TODO: To change to respective version!\n",
    "output_file = os.path.join(dir_path, f\"final_flattened_trl_format_prm_training_data_500k_{mc_score}_v2.jsonl\")\n",
    "print(f\"\\nWriting flattened data to {output_file}...\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Flattened {len(all_data)} total records to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a02f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push flattened JSONL path to S3 path\n",
    "# !aws s3 sync ./s3_upload_prm_training_data/prm_training_data_full_v1 s3://arf-share/arf-ob1-mm-reasoning/prm_training_data_full_v1/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e464e3",
   "metadata": {},
   "source": [
    "✓ Flattened 281334 total records to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/train/mc0.8/final_flattened_trl_format_prm_training_data_500k_mc0.8_v1.jsonl\n",
    "\n",
    "✓ Flattened 284787 total records to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/train/mc0.5/final_flattened_trl_format_prm_training_data_500k_mc0.5_v1.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42becd0a",
   "metadata": {},
   "source": [
    "✓ Flattened 290006 total records to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/train/mc0.0/final_flattened_trl_format_prm_training_data_500k_mc0.0_v2.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
