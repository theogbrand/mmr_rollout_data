{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ace7f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'image_url', 'conversations', 'first_incorrect_step', 'steps_with_score', 'consensus_filtering_algo_label', 'verifier_identified_first_incorrect_step_solution']\n"
     ]
    }
   ],
   "source": [
    "# load all JSONL files in the directory\n",
    "import os\n",
    "import json\n",
    "\n",
    "dir_path = \"/mnt/fast10/brandon/mmr_rollout_data/prm_training_data/debug\"\n",
    "\n",
    "first_jsonl = next((f for f in os.listdir(dir_path) if f.endswith(\".jsonl\")), None)\n",
    "if first_jsonl:\n",
    "    with open(os.path.join(dir_path, first_jsonl), \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line:\n",
    "            columns = list(json.loads(first_line).keys())\n",
    "        else:\n",
    "            columns = []\n",
    "else:\n",
    "    columns = []\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "701e5d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking schema consistency...\n",
      "✓ All files have consistent schema\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "# First pass: Check schema consistency across all files\n",
    "print(\"Checking schema consistency...\")\n",
    "file_schemas = {}\n",
    "jsonl_files = [f for f in os.listdir(dir_path) if f.endswith(\".jsonl\")]\n",
    "\n",
    "for file in jsonl_files:\n",
    "    with open(os.path.join(dir_path, file), \"r\") as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line:\n",
    "            schema = set(json.loads(first_line).keys())\n",
    "            file_schemas[file] = schema\n",
    "\n",
    "# Check if all schemas are identical\n",
    "reference_schema = next(iter(file_schemas.values()))\n",
    "inconsistent_files = []\n",
    "for file, schema in file_schemas.items():\n",
    "    if schema != reference_schema:\n",
    "        inconsistent_files.append((file, schema))\n",
    "\n",
    "if inconsistent_files:\n",
    "    print(\"WARNING: Schema inconsistencies found!\")\n",
    "    for file, schema in inconsistent_files:\n",
    "        print(f\"  {file}: {schema}\")\n",
    "        print(f\"    Missing: {reference_schema - schema}\")\n",
    "        print(f\"    Extra: {schema - reference_schema}\")\n",
    "else:\n",
    "    print(\"✓ All files have consistent schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f21d6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 3 files...\n",
      "Processing InfoVQA_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.8.jsonl...\n",
      "  Lines processed: 25557\n",
      "Processing vqav2_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.8.jsonl...\n",
      "  Lines processed: 8169\n",
      "Processing CLEVR_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.8.jsonl...\n",
      "  Lines processed: 27571\n",
      "\n",
      "Writing flattened data to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/debug/flattened_all_data.jsonl...\n",
      "✓ Flattened 61297 total records to /mnt/fast10/brandon/mmr_rollout_data/prm_training_data/debug/flattened_all_data.jsonl\n",
      "\n",
      "============================================================\n",
      "STATISTICS BY FILE\n",
      "============================================================\n",
      "\n",
      "InfoVQA_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.8.jsonl:\n",
      "  Total records: 25557\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 713 (18.6% of incorrect samples)\n",
      "    Visual Elements: 3128 (81.4% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 15901\n",
      "    o4-mini_correct_and_MC_disagrees: 5815\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 3841\n",
      "  Training samples breakdown:\n",
      "    Used for training: 19742 (15901 correct + 3841 incorrect)\n",
      "    Not used for training: 5815 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 80.5% correct, 19.5% incorrect\n",
      "\n",
      "vqav2_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.8.jsonl:\n",
      "  Total records: 8169\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 1023 (41.1% of incorrect samples)\n",
      "    Visual Elements: 1468 (58.9% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 4355\n",
      "    o4-mini_correct_and_MC_disagrees: 1323\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 2491\n",
      "  Training samples breakdown:\n",
      "    Used for training: 6846 (4355 correct + 2491 incorrect)\n",
      "    Not used for training: 1323 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 63.6% correct, 36.4% incorrect\n",
      "\n",
      "CLEVR_final_mc_rollouts_with_all_models_verification_merged_prm_training_data_mc0.8.jsonl:\n",
      "  Total records: 27571\n",
      "  First incorrect step sections:\n",
      "    Reasoning: 395 (3.8% of incorrect samples)\n",
      "    Visual Elements: 9872 (96.2% of incorrect samples)\n",
      "  Consensus filtering labels:\n",
      "    o4-mini_correct_and_MC_agrees: 13416\n",
      "    o4-mini_correct_and_MC_disagrees: 3888\n",
      "    o4-mini_incorrect_and_MC_agrees_and_disagrees: 10267\n",
      "  Training samples breakdown:\n",
      "    Used for training: 23683 (13416 correct + 10267 incorrect)\n",
      "    Not used for training: 3888 (o4-mini_correct_and_MC_disagrees)\n",
      "    Training split: 56.6% correct, 43.4% incorrect\n",
      "\n",
      "============================================================\n",
      "OVERALL STATISTICS\n",
      "============================================================\n",
      "Total records across all files: 61297\n",
      "Overall first incorrect step sections:\n",
      "  Reasoning: 2131 (12.8% of incorrect samples)\n",
      "  Visual Elements: 14468 (87.2% of incorrect samples)\n",
      "Overall consensus filtering labels:\n",
      "  o4-mini_correct_and_MC_agrees: 33672\n",
      "  o4-mini_correct_and_MC_disagrees: 11026\n",
      "  o4-mini_incorrect_and_MC_agrees_and_disagrees: 16599\n",
      "Overall training samples breakdown:\n",
      "  Used for training: 50271 (33672 correct + 16599 incorrect)\n",
      "  Not used for training: 11026 (o4-mini_correct_and_MC_disagrees)\n",
      "  Training split: 67.0% correct, 33.0% incorrect\n"
     ]
    }
   ],
   "source": [
    "# Statistics collection\n",
    "file_stats = {}\n",
    "all_data = []\n",
    "\n",
    "print(f\"\\nProcessing {len(jsonl_files)} files...\")\n",
    "\n",
    "for file in jsonl_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    \n",
    "    # Initialize stats for this file\n",
    "    first_incorrect_step_sections = Counter()\n",
    "    consensus_filtering_labels = Counter()\n",
    "    line_count = 0\n",
    "    \n",
    "    with open(os.path.join(dir_path, file), \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            all_data.append(item)\n",
    "            line_count += 1\n",
    "            \n",
    "            # Collect first_incorrect_step section distribution\n",
    "            if \"first_incorrect_step\" in item and item[\"first_incorrect_step\"] is not None:\n",
    "                if isinstance(item[\"first_incorrect_step\"], (list, tuple)) and len(item[\"first_incorrect_step\"]) >= 1:\n",
    "                    section_name = item[\"first_incorrect_step\"][0]\n",
    "                    first_incorrect_step_sections[section_name] += 1\n",
    "            \n",
    "            # Collect consensus_filtering_algo_label distribution\n",
    "            if \"consensus_filtering_algo_label\" in item and item[\"consensus_filtering_algo_label\"] is not None:\n",
    "                consensus_filtering_labels[item[\"consensus_filtering_algo_label\"]] += 1\n",
    "    \n",
    "    # Store stats for this file\n",
    "    file_stats[file] = {\n",
    "        \"line_count\": line_count,\n",
    "        \"first_incorrect_step_sections\": dict(first_incorrect_step_sections),\n",
    "        \"consensus_filtering_labels\": dict(consensus_filtering_labels)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Lines processed: {line_count}\")\n",
    "\n",
    "# Write flattened data to single file\n",
    "output_file = os.path.join(dir_path, \"flattened_all_data.jsonl\")\n",
    "print(f\"\\nWriting flattened data to {output_file}...\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for item in all_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"✓ Flattened {len(all_data)} total records to {output_file}\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICS BY FILE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for file, stats in file_stats.items():\n",
    "    print(f\"\\n{file}:\")\n",
    "    print(f\"  Total records: {stats['line_count']}\")\n",
    "    \n",
    "    # Get key counts for percentage calculations\n",
    "    incorrect_count = stats['consensus_filtering_labels'].get('o4-mini_incorrect_and_MC_agrees_and_disagrees', 0)\n",
    "    correct_count = stats['consensus_filtering_labels'].get('o4-mini_correct_and_MC_agrees', 0)\n",
    "    unused_count = stats['consensus_filtering_labels'].get('o4-mini_correct_and_MC_disagrees', 0)\n",
    "    \n",
    "    reasoning_count = stats['first_incorrect_step_sections'].get('Reasoning', 0)\n",
    "    visual_count = stats['first_incorrect_step_sections'].get('Visual Elements', 0)\n",
    "    \n",
    "    print(f\"  First incorrect step sections:\")\n",
    "    for section, count in sorted(stats['first_incorrect_step_sections'].items()):\n",
    "        if incorrect_count > 0:\n",
    "            pct = (count / incorrect_count) * 100\n",
    "            print(f\"    {section}: {count} ({pct:.1f}% of incorrect samples)\")\n",
    "        else:\n",
    "            print(f\"    {section}: {count} (no incorrect samples)\")\n",
    "    \n",
    "    print(f\"  Consensus filtering labels:\")\n",
    "    for label, count in sorted(stats['consensus_filtering_labels'].items()):\n",
    "        print(f\"    {label}: {count}\")\n",
    "    \n",
    "    # Training sample breakdown\n",
    "    training_total = correct_count + incorrect_count\n",
    "    print(f\"  Training samples breakdown:\")\n",
    "    print(f\"    Used for training: {training_total} ({correct_count} correct + {incorrect_count} incorrect)\")\n",
    "    print(f\"    Not used for training: {unused_count} (o4-mini_correct_and_MC_disagrees)\")\n",
    "    \n",
    "    if training_total > 0:\n",
    "        correct_pct = (correct_count / training_total) * 100\n",
    "        incorrect_pct = (incorrect_count / training_total) * 100\n",
    "        print(f\"    Training split: {correct_pct:.1f}% correct, {incorrect_pct:.1f}% incorrect\")\n",
    "    else:\n",
    "        print(f\"    Training split: No training samples\")\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_sections = Counter()\n",
    "all_labels = Counter()\n",
    "\n",
    "for stats in file_stats.values():\n",
    "    for section, count in stats['first_incorrect_step_sections'].items():\n",
    "        all_sections[section] += count\n",
    "    for label, count in stats['consensus_filtering_labels'].items():\n",
    "        all_labels[label] += count\n",
    "\n",
    "print(f\"Total records across all files: {len(all_data)}\")\n",
    "\n",
    "# Overall key counts for percentage calculations\n",
    "overall_incorrect_count = all_labels.get('o4-mini_incorrect_and_MC_agrees_and_disagrees', 0)\n",
    "overall_correct_count = all_labels.get('o4-mini_correct_and_MC_agrees', 0)\n",
    "overall_unused_count = all_labels.get('o4-mini_correct_and_MC_disagrees', 0)\n",
    "\n",
    "print(f\"Overall first incorrect step sections:\")\n",
    "for section, count in sorted(all_sections.items()):\n",
    "    if overall_incorrect_count > 0:\n",
    "        pct = (count / overall_incorrect_count) * 100\n",
    "        print(f\"  {section}: {count} ({pct:.1f}% of incorrect samples)\")\n",
    "    else:\n",
    "        print(f\"  {section}: {count} (no incorrect samples)\")\n",
    "\n",
    "print(f\"Overall consensus filtering labels:\")\n",
    "for label, count in sorted(all_labels.items()):\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "# Overall training sample breakdown\n",
    "overall_training_total = overall_correct_count + overall_incorrect_count\n",
    "print(f\"Overall training samples breakdown:\")\n",
    "print(f\"  Used for training: {overall_training_total} ({overall_correct_count} correct + {overall_incorrect_count} incorrect)\")\n",
    "print(f\"  Not used for training: {overall_unused_count} (o4-mini_correct_and_MC_disagrees)\")\n",
    "\n",
    "if overall_training_total > 0:\n",
    "    overall_correct_pct = (overall_correct_count / overall_training_total) * 100\n",
    "    overall_incorrect_pct = (overall_incorrect_count / overall_training_total) * 100\n",
    "    print(f\"  Training split: {overall_correct_pct:.1f}% correct, {overall_incorrect_pct:.1f}% incorrect\")\n",
    "else:\n",
    "    print(f\"  Training split: No training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd43cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
